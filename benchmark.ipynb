{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eafa587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello again?\n"
     ]
    }
   ],
   "source": [
    "print(\"hello again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source .venv/bin/activate\n",
    "# .venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa26531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing libraries...\n",
      "PyTorch version: 2.10.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "print(\"importing libraries...\")\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6736e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "# cuda 사용 가능 여부 확인\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Info ==========\n",
      "Platform       : macOS-26.2-arm64-arm-64bit\n",
      "Python         : 3.12.12\n",
      "PyTorch        : 2.10.0\n",
      "MPS built      : True\n",
      "MPS available  : True\n",
      "CPU threads    : 4 (intra-op)\n",
      "Interop threads: 8\n",
      "=================================\n",
      "\n",
      "--- Benchmark start: device=mps, size=4096, dtype=torch.float16, seconds=10.0 ---\n",
      "[  1.04s] iters/sec:    14.48 | est TFLOPS:   1.99\n",
      "[  2.00s] iters/sec:    14.45 | est TFLOPS:   1.99\n",
      "[  3.04s] iters/sec:    14.47 | est TFLOPS:   1.99\n",
      "[  4.01s] iters/sec:    14.42 | est TFLOPS:   1.98\n",
      "[  5.05s] iters/sec:    14.45 | est TFLOPS:   1.99\n",
      "[  6.02s] iters/sec:    14.45 | est TFLOPS:   1.99\n",
      "[  7.06s] iters/sec:    14.45 | est TFLOPS:   1.99\n",
      "[  8.03s] iters/sec:    14.45 | est TFLOPS:   1.99\n",
      "[  9.06s] iters/sec:    14.46 | est TFLOPS:   1.99\n",
      "[ 10.03s] iters/sec:    14.45 | est TFLOPS:   1.99\n",
      "\n",
      "========== Summary ==========\n",
      "Device             : mps\n",
      "Total time         : 10.032 sec\n",
      "Total iters(matmul): 145\n",
      "FLOPs per iter     : 0.137456 TFLOPs (matmul 0.137439 + relu 0.000017)\n",
      "Total FLOPs        : 19.931 TFLOPs\n",
      "Achieved TFLOPS    : 1.987\n",
      "=============================\n",
      "\n",
      "--- Benchmark start: device=cpu, size=2048, dtype=torch.float32, seconds=10.0 ---\n",
      "[  1.00s] iters/sec:    50.97 | est TFLOPS:   0.88\n",
      "[  2.00s] iters/sec:    50.89 | est TFLOPS:   0.87\n",
      "[  3.01s] iters/sec:    50.78 | est TFLOPS:   0.87\n",
      "[  4.00s] iters/sec:    51.17 | est TFLOPS:   0.88\n",
      "[  5.02s] iters/sec:    51.25 | est TFLOPS:   0.88\n",
      "[  6.01s] iters/sec:    51.30 | est TFLOPS:   0.88\n",
      "[  7.01s] iters/sec:    51.09 | est TFLOPS:   0.88\n",
      "[  8.01s] iters/sec:    51.27 | est TFLOPS:   0.88\n",
      "[  9.01s] iters/sec:    51.02 | est TFLOPS:   0.88\n",
      "[ 10.02s] iters/sec:    51.37 | est TFLOPS:   0.88\n",
      "\n",
      "========== Summary ==========\n",
      "Device             : cpu\n",
      "Total time         : 10.018 sec\n",
      "Total iters(matmul): 512\n",
      "FLOPs per iter     : 0.017184 TFLOPs (matmul 0.017180 + relu 0.000004)\n",
      "Total FLOPs        : 8.798 TFLOPs\n",
      "Achieved TFLOPS    : 0.878\n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mac 버전\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "\n",
    "def device_sync(device: torch.device, tensor_for_sync: torch.Tensor | None = None) -> None:\n",
    "    \"\"\"\n",
    "    장치별 동기화.\n",
    "    - CUDA: torch.cuda.synchronize()\n",
    "    - MPS: torch.mps.synchronize() (가능한 경우)\n",
    "    - fallback: tensor_for_sync를 CPU로 한번 가져와 완료를 강제\n",
    "    \"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        return\n",
    "\n",
    "    if device.type == \"mps\":\n",
    "        # PyTorch 버전에 따라 torch.mps.synchronize()가 있을 수 있음\n",
    "        if hasattr(torch, \"mps\") and hasattr(torch.mps, \"synchronize\"):\n",
    "            torch.mps.synchronize()\n",
    "            return\n",
    "\n",
    "    # CPU 또는 기타: 특별한 동기화 필요 없음\n",
    "    # 다만 MPS에서 synchronize가 없다면 fallback으로 결과를 CPU로 가져와 완료를 강제\n",
    "    if device.type == \"mps\" and tensor_for_sync is not None:\n",
    "        _ = tensor_for_sync.float().mean().cpu().item()\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(\"========== System Info ==========\")\n",
    "    print(f\"Platform       : {platform.platform()}\")\n",
    "    print(f\"Python         : {platform.python_version()}\")\n",
    "    print(f\"PyTorch        : {torch.__version__}\")\n",
    "    print(f\"MPS built      : {torch.backends.mps.is_built()}\")\n",
    "    print(f\"MPS available  : {torch.backends.mps.is_available()}\")\n",
    "    print(f\"CPU threads    : {torch.get_num_threads()} (intra-op)\")\n",
    "    print(f\"Interop threads: {torch.get_num_interop_threads()}\")\n",
    "    print(\"=================================\\n\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def gemm_bench_exact(\n",
    "    device: torch.device,\n",
    "    seconds: float = 10.0,\n",
    "    size: int = 4096,\n",
    "    dtype: torch.dtype = torch.float16,\n",
    "    use_relu: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    정확한 시간 측정을 위해 각 반복마다 device_sync를 호출해 backlog를 막는다.\n",
    "    - 매초 matmul/sec, (추정)TFLOPS 출력\n",
    "    - 마지막에 총 FLOPs 및 평균 처리량 출력\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"--- Benchmark start: device={device}, size={size}, dtype={dtype}, seconds={seconds} ---\")\n",
    "\n",
    "    # dtype 안전장치: MPS는 float16을 지원하지만, 일부 연산은 float32가 더 안정적일 수 있음\n",
    "    # 사용자가 원하는 dtype을 그대로 쓰되, 문제 생기면 float32로 바꿔 테스트하세요.\n",
    "\n",
    "    a = torch.randn((size, size), device=device, dtype=dtype)\n",
    "    b = torch.randn((size, size), device=device, dtype=dtype)\n",
    "\n",
    "    # 워밍업\n",
    "    c = a @ b\n",
    "    if use_relu:\n",
    "        c = c.relu_()\n",
    "    device_sync(device, c)\n",
    "\n",
    "    # FLOPs 모델: GEMM = 2*N^3, ReLU ~ N^2\n",
    "    flops_matmul = 2 * (size ** 3)\n",
    "    flops_relu = (size ** 2) if use_relu else 0\n",
    "    flops_per_iter = flops_matmul + flops_relu\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    next_report = start + 1.0\n",
    "    interval_start = start\n",
    "\n",
    "    total_iters = 0\n",
    "    interval_iters = 0\n",
    "\n",
    "    while True:\n",
    "        # 1) 연산 실행\n",
    "        c = a @ b\n",
    "        if use_relu:\n",
    "            c = c.relu_()\n",
    "\n",
    "        # 2) 완료 동기화(정확 측정 핵심)\n",
    "        device_sync(device, c)\n",
    "\n",
    "        total_iters += 1\n",
    "        interval_iters += 1\n",
    "\n",
    "        now = time.perf_counter()\n",
    "        elapsed = now - start\n",
    "\n",
    "        # 3) 매초 출력\n",
    "        if now >= next_report:\n",
    "            interval = now - interval_start\n",
    "            iters_per_sec = interval_iters / interval\n",
    "            tflops = (flops_per_iter * iters_per_sec) / 1e12\n",
    "\n",
    "            print(f\"[{elapsed:6.2f}s] iters/sec: {iters_per_sec:8.2f} | est TFLOPS: {tflops:6.2f}\")\n",
    "\n",
    "            interval_start = now\n",
    "            interval_iters = 0\n",
    "            while next_report <= now:\n",
    "                next_report += 1.0\n",
    "\n",
    "        # 4) 정확한 종료\n",
    "        if elapsed >= seconds:\n",
    "            break\n",
    "\n",
    "    total_time = time.perf_counter() - start\n",
    "    total_flops = flops_per_iter * total_iters\n",
    "    achieved_tflops = (total_flops / total_time) / 1e12\n",
    "\n",
    "    print(\"\\n========== Summary ==========\")\n",
    "    print(f\"Device             : {device}\")\n",
    "    print(f\"Total time         : {total_time:.3f} sec\")\n",
    "    print(f\"Total iters(matmul): {total_iters}\")\n",
    "    print(f\"FLOPs per iter     : {flops_per_iter/1e12:.6f} TFLOPs \"\n",
    "          f\"(matmul {flops_matmul/1e12:.6f} + relu {flops_relu/1e12:.6f})\")\n",
    "    print(f\"Total FLOPs        : {total_flops/1e12:.3f} TFLOPs\")\n",
    "    print(f\"Achieved TFLOPS    : {achieved_tflops:.3f}\")\n",
    "    print(\"=============================\\n\")\n",
    "\n",
    "    return {\n",
    "        \"device\": str(device),\n",
    "        \"size\": size,\n",
    "        \"dtype\": str(dtype),\n",
    "        \"seconds\": seconds,\n",
    "        \"total_time\": total_time,\n",
    "        \"iters\": total_iters,\n",
    "        \"achieved_tflops\": achieved_tflops,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    print_system_info()\n",
    "\n",
    "    # 1) MPS 가능하면 MPS 먼저\n",
    "    if torch.backends.mps.is_available():\n",
    "        # M1에서는 4096~8192 사이가 흔히 잘 맞습니다. 메모리(통합) 여유에 따라 조절.\n",
    "        gemm_bench_exact(device=torch.device(\"mps\"), seconds=10.0, size=4096, dtype=torch.float16, use_relu=True)\n",
    "    else:\n",
    "        print(\"MPS가 사용 불가합니다. CPU만 측정합니다.\\n\")\n",
    "\n",
    "    # 2) CPU 측정\n",
    "    # CPU는 float32가 일반적\n",
    "    gemm_bench_exact(device=torch.device(\"cpu\"), seconds=10.0, size=2048, dtype=torch.float32, use_relu=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48df132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Info ==========\n",
      "Platform       : Windows-10-10.0.19045-SP0\n",
      "Python         : 3.12.12\n",
      "PyTorch        : 2.10.0+cpu\n",
      "CUDA available : False\n",
      "CPU threads    : 4 (intra-op)\n",
      "Interop threads: 4\n",
      "=================================\n",
      "\n",
      "CUDA GPU가 없어 GPU 벤치를 건너뜁니다.\n",
      "\n",
      "--- Benchmark: device=cpu, size=2048, dtype=torch.float32, seconds=10.0 ---\n",
      "Extra op: relu\n",
      "\n",
      "[  1.13s] iters/sec:     3.55 | est TFLOPS:   0.06\n",
      "[  2.06s] iters/sec:     3.20 | est TFLOPS:   0.06\n",
      "[  3.20s] iters/sec:     3.52 | est TFLOPS:   0.06\n",
      "[  4.03s] iters/sec:     3.63 | est TFLOPS:   0.06\n",
      "[  5.11s] iters/sec:     3.67 | est TFLOPS:   0.06\n",
      "[  6.20s] iters/sec:     2.75 | est TFLOPS:   0.05\n",
      "[  7.28s] iters/sec:     2.78 | est TFLOPS:   0.05\n",
      "[  8.04s] iters/sec:     2.64 | est TFLOPS:   0.05\n",
      "[  9.15s] iters/sec:     2.70 | est TFLOPS:   0.05\n",
      "[ 10.12s] iters/sec:     3.09 | est TFLOPS:   0.05\n",
      "\n",
      "========== Summary ==========\n",
      "Device          : cpu\n",
      "Total time      : 10.124 sec\n",
      "Total iters     : 32\n",
      "Total FLOPs     : 0.550 TFLOPs\n",
      "Achieved TFLOPS : 0.054\n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window 버전\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(\"========== System Info ==========\")\n",
    "    print(f\"Platform       : {platform.platform()}\")\n",
    "    print(f\"Python         : {platform.python_version()}\")\n",
    "    print(f\"PyTorch        : {torch.__version__}\")\n",
    "    print(f\"CUDA available : {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        idx = torch.cuda.current_device()\n",
    "        props = torch.cuda.get_device_properties(idx)\n",
    "        print(f\"GPU            : cuda:{idx} ({props.name})\")\n",
    "        print(f\"Compute cap    : {props.major}.{props.minor}\")\n",
    "        print(f\"VRAM           : {props.total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"CUDA version   : {torch.version.cuda}\")\n",
    "    print(f\"CPU threads    : {torch.get_num_threads()} (intra-op)\")\n",
    "    print(f\"Interop threads: {torch.get_num_interop_threads()}\")\n",
    "    print(\"=================================\\n\")\n",
    "\n",
    "\n",
    "def device_sync(device: torch.device):\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def gemm_bench_exact(\n",
    "    device: torch.device,\n",
    "    seconds: float = 10.0,\n",
    "    size: int = 8192,\n",
    "    dtype: torch.dtype = torch.float16,\n",
    "    use_relu: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    - CUDA: 1초마다 synchronize해서 backlog를 과도하게 쌓지 않으면서 측정 안정화\n",
    "    - CPU: 그대로 측정\n",
    "    - 정확히 seconds 동안 실행(루프 종료는 wall-clock 기준), 마지막에 sync 후 요약\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"--- Benchmark: device={device}, size={size}, dtype={dtype}, seconds={seconds} ---\")\n",
    "    print(f\"Extra op: {'relu' if use_relu else 'none'}\\n\")\n",
    "\n",
    "    a = torch.randn((size, size), device=device, dtype=dtype)\n",
    "    b = torch.randn((size, size), device=device, dtype=dtype)\n",
    "\n",
    "    # warmup\n",
    "    c = a @ b\n",
    "    if use_relu:\n",
    "        c.relu_()\n",
    "    device_sync(device)\n",
    "\n",
    "    # FLOPs 모델: GEMM=2*N^3, ReLU~N^2\n",
    "    flops_matmul = 2 * (size ** 3)\n",
    "    flops_relu = (size ** 2) if use_relu else 0\n",
    "    flops_per_iter = flops_matmul + flops_relu\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    end_time = start + seconds\n",
    "\n",
    "    next_report = start + 1.0\n",
    "    interval_start = start\n",
    "    interval_iters = 0\n",
    "    total_iters = 0\n",
    "\n",
    "    while True:\n",
    "        c = a @ b\n",
    "        if use_relu:\n",
    "            c.relu_()\n",
    "\n",
    "        total_iters += 1\n",
    "        interval_iters += 1\n",
    "\n",
    "        now = time.perf_counter()\n",
    "\n",
    "        # 1초마다 출력 (CUDA는 여기서만 sync해서 측정 정확도/오버헤드 균형)\n",
    "        if now >= next_report:\n",
    "            device_sync(device)\n",
    "            t = time.perf_counter()\n",
    "            interval = t - interval_start\n",
    "\n",
    "            iters_per_sec = interval_iters / interval\n",
    "            tflops = (flops_per_iter * iters_per_sec) / 1e12\n",
    "            elapsed = t - start\n",
    "\n",
    "            print(f\"[{elapsed:6.2f}s] iters/sec: {iters_per_sec:8.2f} | est TFLOPS: {tflops:6.2f}\")\n",
    "\n",
    "            interval_start = t\n",
    "            interval_iters = 0\n",
    "            while next_report <= now:\n",
    "                next_report += 1.0\n",
    "\n",
    "        if now >= end_time:\n",
    "            break\n",
    "\n",
    "    # 마무리\n",
    "    device_sync(device)\n",
    "    total_time = time.perf_counter() - start\n",
    "\n",
    "    total_flops = flops_per_iter * total_iters\n",
    "    achieved_tflops = (total_flops / total_time) / 1e12\n",
    "\n",
    "    print(\"\\n========== Summary ==========\")\n",
    "    print(f\"Device          : {device}\")\n",
    "    print(f\"Total time      : {total_time:.3f} sec\")\n",
    "    print(f\"Total iters     : {total_iters}\")\n",
    "    print(f\"Total FLOPs     : {total_flops/1e12:.3f} TFLOPs\")\n",
    "    print(f\"Achieved TFLOPS : {achieved_tflops:.3f}\")\n",
    "    print(\"=============================\\n\")\n",
    "\n",
    "    return {\n",
    "        \"device\": str(device),\n",
    "        \"size\": size,\n",
    "        \"dtype\": str(dtype),\n",
    "        \"seconds\": seconds,\n",
    "        \"total_time\": total_time,\n",
    "        \"iters\": total_iters,\n",
    "        \"achieved_tflops\": achieved_tflops,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    print_system_info()\n",
    "\n",
    "    # CUDA GPU 벤치 (가능한 경우)\n",
    "    if torch.cuda.is_available():\n",
    "        # VRAM 8GB급이면 8192 FP16이 빡셀 수 있어 6144부터 권장\n",
    "        gemm_bench_exact(\n",
    "            device=torch.device(\"cuda\"),\n",
    "            seconds=10.0,\n",
    "            size=8192,          # OOM 나면 6144 또는 4096로\n",
    "            dtype=torch.float16,\n",
    "            use_relu=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"CUDA GPU가 없어 GPU 벤치를 건너뜁니다.\\n\")\n",
    "\n",
    "    # CPU 벤치\n",
    "    gemm_bench_exact(\n",
    "        device=torch.device(\"cpu\"),\n",
    "        seconds=10.0,\n",
    "        size=2048,\n",
    "        dtype=torch.float32,\n",
    "        use_relu=True\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fab512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
